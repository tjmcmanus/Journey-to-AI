{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trading Platform Customer Attrition Risk Prediction using sklearn\n",
    "\n",
    "There are many users of online trading platforms and these companies would like to run analytics on and predict churn based on user activity on the platform. Since competition is rife, keeping customers happy so they do not move their investments elsewhere is key to maintaining profitability.\n",
    "\n",
    "In this notebook, we will leverage Watson Studio Local (that is a service on IBM Cloud Pak for Data) to do the following:\n",
    "\n",
    "1. Ingest merged customer demographics and trading activity data\n",
    "2. Visualize merged dataset and get better understanding of data to build hypotheses for prediction\n",
    "3. Leverage sklearn library to build classification model that predicts whether customer has propensity to churn\n",
    "4. Expose the classification model as RESTful API endpoint for the end-to-end customer churn risk prediction and risk remediation application\n",
    "\n",
    "<img src=\"https://github.com/burtvialpando/CloudPakWorkshop/blob/master/CPD/images/NotebookImage.png?raw=true\" width=\"800\" height=\"500\" align=\"middle\"/>\n",
    "\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load libraries](#load_libraries)\n",
    "2. [Load and visualize merged customer demographics and trading activity data](#load_data)\n",
    "3. [Prepare data for building classification model](#prepare_data)\n",
    "4. [Train classification model and test model performance](#build_model)\n",
    "5. [Save model to ML repository and expose it as REST API endpoint](#save_model)\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick set of instructions to work through the notebook\n",
    "\n",
    "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n",
    "\n",
    "1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n",
    "2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n",
    "3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n",
    "4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_libraries\"></a>\n",
    "## 1. Load libraries\n",
    "[Top](#top)\n",
    "\n",
    "Running the following cell will load all libraries needed to load, visualize, prepare the data and build ML models for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and run once to install the package in your runtime environment\n",
    "!pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following cell doesn't work, please un-comment out the next line and do upgrade the patplotlib package. When the upgrade is done, restart the kernal and start from the beginning again. \n",
    "#!pip install --user --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brunel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changed sk-learn version to be compatible with WML client4 on cpd 3.0.1\n",
    "!pip install scikit-learn==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"load_data\"></a>\n",
    "## 2. Load data example\n",
    "[Top](#top)\n",
    "\n",
    "Data can be easily loaded within ICPD using point-and-click functionality. The following image illustrates how to load a merged dataset assuming it is called \"customer_demochurn_activity_analyze.csv\". The file can be located by its name and inserted into the notebook as a **pandas** dataframe as shown below:\n",
    "\n",
    "<img src=\"https://github.com/burtvialpando/CloudPakWorkshop/blob/master/CPD/images/InsertPandasDataFrame.png?raw=true\" width=\"300\" height=\"300\" align=\"middle\"/>\n",
    "\n",
    "The interface comes up with a generic name, so it is good practice to rename the dataframe to match context of the use case. In this case, we will use df_churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_churn_pd = pd.read_csv('/project_data/data_asset/customer_demochurn_activity_analyze.csv')\n",
    "df_churn_pd.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization is key step in data mining process that helps better understand data before it can be prepared for building ML models\n",
    "\n",
    "We use Brunel library that comes preloaded within Watson Studio local environment to visualize the merged customer data. \n",
    "\n",
    "The Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n",
    "\n",
    "Try Brunel visualization here: http://brunel.mybluemix.net/gallery_app/renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%brunel data('df_churn_pd') stack polar bar x(CHURNRISK) y(#count) color(CHURNRISK) bar tooltip(#all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%brunel data('df_churn_pd') bar x(STATUS) y(#count) color(STATUS) tooltip(#all) | stack bar x(STATUS) y(#count) color(CHURNRISK: pink-orange-yellow) bin(STATUS) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%brunel data('df_churn_pd') bar x(TOTALUNITSTRADED) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%brunel data('df_churn_pd') bar x(DAYSSINCELASTTRADE) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare_data\"></a>\n",
    "## 3. Data preparation\n",
    "[Top](#top)\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices and encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n",
    "\n",
    "Final step in the data preparation process is to assemble all the categorical and non-categorical columns into a feature vector. We use VectorAssembler for this. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the DataFrameMapper class to declare transformations and variable imputations.\n",
    "\n",
    "* LabelBinarizer - Converts a categorical variable into a dummy variable (aka binary variable)\n",
    "* StandardScaler - Standardize features by removing the mean and scaling to unit variance, z = (x - u) / s\n",
    "\n",
    "See docs: \n",
    "* https://github.com/scikit-learn-contrib/sklearn-pandas\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns \n",
    "categoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER', 'AGE_GROUP']\n",
    "numericColumns = ['CHILDREN', 'ESTINCOME', 'TOTALDOLLARVALUETRADED', 'TOTALUNITSTRADED', 'LARGESTSINGLETRANSACTION', 'SMALLESTSINGLETRANSACTION', \n",
    "                          'PERCENTCHANGECALCULATION', 'DAYSSINCELASTLOGIN', 'DAYSSINCELASTTRADE', 'NETREALIZEDGAINS_YTD', 'NETREALIZEDLOSSES_YTD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['GENDER'], LabelBinarizer()),\n",
    "    (['STATUS'], LabelBinarizer()),\n",
    "    (['HOMEOWNER'], LabelBinarizer()),\n",
    "    (['AGE_GROUP'], LabelBinarizer()),\n",
    "    (['CHILDREN'],  StandardScaler()),\n",
    "    (['ESTINCOME'],  StandardScaler()),\n",
    "    (['TOTALDOLLARVALUETRADED'],  StandardScaler()),\n",
    "    (['TOTALUNITSTRADED'],  StandardScaler()),\n",
    "    (['LARGESTSINGLETRANSACTION'],  StandardScaler()),\n",
    "    (['SMALLESTSINGLETRANSACTION'],  StandardScaler()),\n",
    "    (['PERCENTCHANGECALCULATION'],  StandardScaler()),\n",
    "    (['DAYSSINCELASTLOGIN'],  StandardScaler()),\n",
    "    (['DAYSSINCELASTTRADE'],  StandardScaler()),\n",
    "    (['NETREALIZEDGAINS_YTD'],  StandardScaler()),\n",
    "    (['NETREALIZEDLOSSES_YTD'],  StandardScaler())], default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data to the model\n",
    "X = df_churn_pd.drop(['ID','CHURNRISK','AGE','TAXID','CREDITCARD','DOB','ADDRESS_1', 'ADDRESS_2', 'CITY', 'STATE', 'ZIP', 'ZIP4', 'LONGITUDE',\n",
    "       'LATITUDE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable and encode with value between 0 and n_classes-1\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_churn_pd['CHURNRISK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build_model\"></a>\n",
    "## 4. Build Random Forest classification model\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a decision-tree based classification algorithm, namely, RandomForestClassifier. Next we define a pipeline to chain together the various transformers and estimaters defined during the data preparation step before. Sklearn standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n",
    "\n",
    "We split original dataset into train and test datasets. We fit the pipeline to training data and apply the trained model to transform test data and generate churn risk class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Classifier\n",
    "random_forest = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Define the steps in the pipeline to sequentially apply a list of transforms and the estimator, i.e. RandomForestClassifier\n",
    "steps = [('mapper', mapper),('RandonForestClassifier', random_forest)]\n",
    "pipeline = sklearn.pipeline.Pipeline(steps)\n",
    "\n",
    "# train the model\n",
    "model=pipeline.fit( X_train, y_train )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "y_prediction = model.predict( X_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 10 rows of predictions\n",
    "y_prediction[0:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 10 rows of predictions with the corresponding labels\n",
    "le.inverse_transform(y_prediction)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model results\n",
    "\n",
    "In a supervised classification problem such as churn risk classification, we have a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n",
    "\n",
    "1. True Positive (TP) - label is positive and prediction is also positive\n",
    "2. True Negative (TN) - label is negative and prediction is also negative\n",
    "3. False Positive (FP) - label is negative but prediction is positive\n",
    "4. False Negative (FN) - label is positive but prediction is negative\n",
    "\n",
    "These four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering classifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The reason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95% of the data points are not fraud and 5% of the data points are fraud, then a naive classifier that predicts not fraud, regardless of input, will be 95% accurate. For this reason, metrics like precision and recall are typically used because they take into account the type of error. In most applications there is some desired balance between precision and recall, which can be captured by combining the two into a single metric, called the F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display label mapping to assist with interpretation of the model results\n",
    "label_mapping=le.inverse_transform([0,1,2])\n",
    "print('0: ', label_mapping[0])\n",
    "print('1: ', label_mapping[1])\n",
    "print('2: ', label_mapping[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test your predictions using sklearn.classification_report()\n",
    "report = sklearn.metrics.classification_report( y_test, y_prediction )\n",
    "\n",
    "### and print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:   ',sklearn.metrics.accuracy_score( y_test, y_prediction ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the column names of the transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_step=pipeline.named_steps['mapper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_step.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = m_step.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features importance\n",
    "importances = pipeline.named_steps['RandonForestClassifier'][1].feature_importances_\n",
    "indices = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b',align='center')\n",
    "plt.yticks(range(len(indices)), (np.array(features))[indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save_model\"></a>\n",
    "## 5. Save the model into WML Deployment Space\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save the model we must create a deployment space. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their models. We can save models, functions and data assets in this space.\n",
    "\n",
    "The steps involved for saving and deploying the model are as follows:\n",
    "\n",
    "1. Create a new deployment space. Enter the name of the space in the cell below. If a space with specified space_name already exists, existing space will be deleted before creating a new space.\n",
    "2. Set this deployment space as the default space.\n",
    "3. Store the model pipeline in the deployment space. Enter the name for the model in the cell below. \n",
    "4. Deploy the saved model. Enter the deployment name in the cell below. \n",
    "5. Retrieve the scoring endpoint to score the model with a payload\n",
    "6. We will use the watson_machine_learning_client package to complete these steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a names for the space being created, the saved model and the model deployment\n",
    "space_name = 'deployment-space-analytics-project-workshop'\n",
    "\n",
    "model_name = 'churn_risk_model'\n",
    "\n",
    "deployment_name = 'churn_risk_model-deployment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "import os\n",
    "token = os.environ['USER_ACCESS_TOKEN']\n",
    "\n",
    "from project_lib.utils import environment\n",
    "url = environment.get_common_api_url()\n",
    "\n",
    "wml_credentials = {\n",
    "\"token\": token,\n",
    "\"instance_id\" : \"wml_local\",\n",
    "\"url\": url,\n",
    "\"version\": \"3.0.0\"\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a space with specified space_name already exists, delete the existing space before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for space in client.spaces.get_details()['resources']:\n",
    "    if space_name in space['entity']['name']:\n",
    "        client.spaces.delete(space['metadata']['guid'])\n",
    "        print(space_name, \"is deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create Deployment Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the space and set it as default\n",
    "space_meta_data = {\n",
    "        client.spaces.ConfigurationMetaNames.NAME : space_name\n",
    "}\n",
    "\n",
    "stored_space_details = client.spaces.store(space_meta_data)\n",
    "\n",
    "space_uid = stored_space_details['metadata']['guid']\n",
    "\n",
    "# set the newly created deployment space as the default\n",
    "client.set.default_space(space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching details of the space created\n",
    "stored_space_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Store the model in the deployment space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all supported software specs\n",
    "client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this line if you do not know the version of scikit-learn that was used to build the model\n",
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_spec_uid = client.software_specifications.get_uid_by_name('scikit-learn_0.22-py3.6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: model_name,\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n",
    "    client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.22\"\n",
    "}\n",
    "\n",
    "stored_model_details = client.repository.store_model(pipeline,\n",
    "                                               meta_props=metadata,\n",
    "                                               training_data=X_train,\n",
    "                                               training_target=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_model_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create a deployment for the stored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the model\n",
    "meta_props = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: deployment_name,\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "# deploy the model\n",
    "\n",
    "model_uid = stored_model_details[\"metadata\"][\"guid\"]\n",
    "deployment_details = client.deployments.create( artifact_uid=model_uid, meta_props=meta_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the scoring endpoint\n",
    "scoring_endpoint = client.deployments.get_scoring_href(deployment_details)\n",
    "\n",
    "print('Scoring Endpoint:   ',scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_deployment_id = client.deployments.get_uid(deployment_details)\n",
    "client.deployments.get_details(scoring_deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = [{\"values\": [ ['Young adult','M','S', 2,56000, 'N', 5030, 23, 2257, 125, 3.45, 2, 19, 1200, 251]]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_metadata = {client.deployments.ScoringMetaNames.INPUT_DATA: payload}\n",
    "# score\n",
    "predictions = client.deployments.score(scoring_deployment_id, payload_metadata)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display label mapping to assist with interpretation of the model results\n",
    "label_mapping=le.inverse_transform([0,1,2])\n",
    "print('0: ', label_mapping[0])\n",
    "print('1: ', label_mapping[1])\n",
    "print('2: ', label_mapping[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write test data into .csv files for batch scoring and model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test data a .csv so that we can later use it for batch scoring\n",
    "write_score_CSV=X_test\n",
    "write_score_CSV.to_csv('/project_data/data_asset/model_batch_score.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test data to a .csv so that we can later use it for Evaluation\n",
    "write_eval_CSV=X_test\n",
    "write_eval_CSV.to_csv('/project_data/data_asset/model_eval.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last updated:** 06/01/2020 - Original Notebook by Anjali Shah, updated in later versions by Sidney Phoon. Final edits by Burt Vialpando and Kent Rubin - IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
